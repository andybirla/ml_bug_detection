text_file = open("Natural_Language_Processing_Text.txt", "Natural_Language_Processing_Text.txt")
text = text_file.read(, )
sentences = sent_tokenize(text, text)
words = word_tokenize(text, text)
fdist = FreqDist(words, words)
fdist.most_common(10, 10)
fdist.plot(10, 10)
words_no_punc = [, ]
    if w.isalpha(, ):
        words_no_punc.append(w.lower(, w.lower())
fdist = FreqDist(words_no_punc, words_no_punc)
fdist.most_common(10, 10)
fdist.plot(10, 10)
stopwords = stopwords.words("english", "english")
clean_words = [, ]
        clean_words.append(w, w)
fdist = FreqDist(clean_words, clean_words)
fdist.most_common(10, 10)
fdist.plot(10, 10)
wordcloud = WordCloud(, ).generate(text)
plt.figure(figsize = (12, 12, figsize = (12)) 
plt.imshow(wordcloud, wordcloud) 
plt.axis("off", "off") 
plt.show(, )
char_mask = np.array(Image.open("circle.png", Image.open("circle.png"))
wordcloud = WordCloud(background_color="black",mask=char_mask, background_color="black").generate(text)
plt.figure(figsize = (8,8, figsize = (8))
plt.imshow(wordcloud, wordcloud)
plt.axis("off", "off")
plt.show(, )
porter = PorterStemmer(, )
word_list = ["Study","Studying","Studies","Studied", "Study"]
snowball = SnowballStemmer("english", "english")
word_list = ["Study","Studying","Studies","Studied", "Study"]
lemma = WordNetLemmatizer(, )
word_list = ["Study","Studying","Studies","Studied", "Study"]
lemma = WordNetLemmatizer(, )
word_list = ["am","is","are","was","were", "am"]
stemmer = PorterStemmer(, )
lemmatizer = WordNetLemmatizer(, )
lemmatizer = WordNetLemmatizer(, )
lemma = WordNetLemmatizer(, )
word_list = ["studies","leaves","decreases","plays", "studies"]
tag = nltk.pos_tag(["Studying","Study"], ["Studying")
tokenized_words = word_tokenize(sentence, sentence)
    tagged_words = nltk.pos_tag(tokenized_words, tokenized_words)
parser = nltk.RegexpParser(grammar, grammar)
output = parser.parse(tagged_words, tagged_words)
parser = nltk.RegexpParser(grammar, grammar)
output = parser.parse(tagged_words, tagged_words)
tokenized_words = word_tokenize(sentence, sentence)
    tagged_words = nltk.pos_tag(tokenized_words, tokenized_words)
N_E_R = nltk.ne_chunk(tagged_words,binary=False, tagged_words)
tokenized_words = word_tokenize(sentence, sentence)
    tagged_words = nltk.pos_tag(tokenized_words, tokenized_words)
N_E_R = nltk.ne_chunk(tagged_words,binary=True, tagged_words)
for words in wordnet.synsets("Fun", "Fun"): 
for words in wordnet.synsets("Fun", "Fun"): 
    for lemma in words.lemmas(, ): 
for words in wordnet.synsets("Fun", "Fun"): 
    for lemma in words.lemmas(, ): 
word = wordnet.synsets("Play", "Play")[0]
word = wordnet.synsets("Play", "Play")[0]
word = wordnet.synsets("Play", "Play")[0]
word.hyponyms(, )
word = wordnet.synsets("Play", "Play")[0]
synonyms = [, ]
for words in wordnet.synsets('Fun', 'Fun'):
    for lemma in words.lemmas(, ):
        synonyms.append(lemma.name(, lemma.name())
antonyms = [, ]
for words in wordnet.synsets('Natural', 'Natural'):
    for lemma in words.lemmas(, ):
        if lemma.antonyms(, ):
            antonyms.append(lemma.antonyms(, lemma.antonyms()[0].name())
synonyms = [, ]
antonyms = [, ]
for words in wordnet.synsets('New', 'New'):
    for lemma in words.lemmas(, ):
        synonyms.append(lemma.name(, lemma.name())
        if lemma.antonyms(, ):
            antonyms.append(lemma.antonyms(, lemma.antonyms()[0].name())
word1 = wordnet.synsets("ship","n", "ship")[0]
word2 = wordnet.synsets("boat","n", "boat")[0] 
word1 = wordnet.synsets("ship","n", "ship")[0]
word2 = wordnet.synsets("bike","n", "bike")[0] 
cv = CountVectorizer(, )
B_O_W = cv.fit_transform(sentences, sentences).toarray()
sentences = ['This is the first document','This document is the second document', 'This is the first document']
vectorizer = TfidfVectorizer(norm = None, norm = None)
X = vectorizer.fit_transform(sentences, sentences).toarray()
