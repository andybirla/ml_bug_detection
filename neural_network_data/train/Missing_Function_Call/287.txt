text_file = ("Natural_Language_Processing_Text.txt")
text = ()
(type(text))
("\n")
(text)
("\n")
(len(text))
sentences = (text)
(len(sentences))
(sentences)
words = (text)
(len(words))
("\n")
(words)
fdist = (words)
(10)
(10)
    if ():
        (w.lower())
(words_no_punc)
("\n")
(len(words_no_punc))
fdist = (words_no_punc)
(10)
(10)
stopwords = ("english")
(stopwords)
        (w)
(clean_words)
("\n")
(len(clean_words))
fdist = (clean_words)
(10)
(10)
wordcloud = ().generate(text)
(figsize = (12, 12)) 
(wordcloud) 
("off") 
()
char_mask = (Image.open("circle.png"))
wordcloud = (background_color="black",mask=char_mask).generate(text)
(figsize = (8,8))
(wordcloud)
("off")
()
porter = ()
    (porter.stem(w))
snowball = ("english")
    (snowball.stem(w))
(SnowballStemmer.languages)
lemma = ()
    (lemma.lemmatize(w ,pos="v"))
lemma = ()
    (lemma.lemmatize(w ,pos="v"))
stemmer = ()
(stemmer.stem('studies'))
lemmatizer = ()
(lemmatizer.lemmatize('studies'))
lemmatizer = ()
(lemmatizer.lemmatize('studying', pos="v"))
(lemmatizer.lemmatize('studying', pos="n"))
(lemmatizer.lemmatize('studying', pos="a"))
(lemmatizer.lemmatize('studying', pos="r"))
lemma = ()
    (lemma.lemmatize(w))
tag = (["Studying","Study"])
(tag)
tokenized_words = (sentence)
    tagged_words = (tokenized_words)
parser = (grammar)
output = (tagged_words)
(output)
parser = (grammar)
output = (tagged_words)
(output)
tokenized_words = (sentence)
    tagged_words = (tokenized_words)
N_E_R = (tagged_words,binary=False)
(N_E_R)
tokenized_words = (sentence)
    tagged_words = (tokenized_words)
N_E_R = (tagged_words,binary=True)
(N_E_R)
for words in ("Fun"): 
    (words)      
for words in ("Fun"): 
    (words.name())
    (words.examples())
    for lemma in (): 
        (lemma)
    ("\n")
for words in ("Fun"): 
    for lemma in (): 
        (lemma)
    ("\n")
word = ("Play")[0]
(word.name())
(word.examples())
word = ("Play")[0]
(word.hypernyms())
word = ("Play")[0]
()
word = ("Play")[0]
(word.lemmas()[0].name())
for words in ('Fun'):
    for lemma in ():
        (lemma.name())
(synonyms)
for words in ('Natural'):
    for lemma in ():
        if ():
            (lemma.antonyms()[0].name())
(antonyms)
for words in ('New'):
    for lemma in ():
        (lemma.name())
        if ():
            (lemma.antonyms()[0].name())
(synonyms)
("\n")
(antonyms)
word1 = ("ship","n")[0]
word2 = ("boat","n")[0] 
(word1.wup_similarity(word2)) 
word1 = ("ship","n")[0]
word2 = ("bike","n")[0] 
(word1.wup_similarity(word2)) 
cv = ()
B_O_W = (sentences).toarray()
(cv.vocabulary_)
("\n")
(cv.get_feature_names())
("\n")
(B_O_W)
vectorizer = (norm = None)
X = (sentences).toarray()
(vectorizer.vocabulary_)
("\n")
(vectorizer.get_feature_names())
("\n")
(X)
