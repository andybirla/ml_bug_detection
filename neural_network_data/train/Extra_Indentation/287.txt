 text_file = open("Natural_Language_Processing_Text.txt")
 text = text_file.read()
 print (type(text))
 print("\n")
 print(text)
 print("\n")
 print (len(text))
 import nltk
 from nltk import sent_tokenize
 from nltk import word_tokenize
 sentences = sent_tokenize(text)
 print (len(sentences))
 print(sentences)
 words = word_tokenize(text)
 print (len(words))
 print("\n")
 print (words)
 from nltk.probability import FreqDist
 fdist = FreqDist(words)
 fdist.most_common(10)
 import matplotlib.pyplot as plt
 fdist.plot(10)
 words_no_punc = []
 for w in words:
     if w.isalpha():
         words_no_punc.append(w.lower())
 print (words_no_punc)
 print ("\n")
 print (len(words_no_punc))
 fdist = FreqDist(words_no_punc)
 fdist.most_common(10)
 fdist.plot(10)
 from nltk.corpus import stopwords
 stopwords = stopwords.words("english")
 print(stopwords)
 clean_words = []
 for w in words_no_punc:
     if w not in stopwords:
         clean_words.append(w)
 print(clean_words)
 print("\n")
 print(len(clean_words))
 fdist = FreqDist(clean_words)
 fdist.most_common(10)
 fdist.plot(10)
 from wordcloud import WordCloud
 import matplotlib.pyplot as plt
 wordcloud = WordCloud().generate(text)
 plt.figure(figsize = (12, 12)) 
 plt.imshow(wordcloud) 
 plt.axis("off") 
 plt.show()
 import numpy as np
 from PIL import Image
 from wordcloud import WordCloud
 char_mask = np.array(Image.open("circle.png"))
 wordcloud = WordCloud(background_color="black",mask=char_mask).generate(text)
 plt.figure(figsize = (8,8))
 plt.imshow(wordcloud)
 plt.axis("off")
 plt.show()
 from nltk.stem import PorterStemmer
 porter = PorterStemmer()
 word_list = ["Study","Studying","Studies","Studied"]
 for w in word_list:
     print(porter.stem(w))
 from nltk.stem import SnowballStemmer
 snowball = SnowballStemmer("english")
 word_list = ["Study","Studying","Studies","Studied"]
 for w in word_list:
     print(snowball.stem(w))
 from nltk.stem import SnowballStemmer
 print(SnowballStemmer.languages)
 from nltk import WordNetLemmatizer
 lemma = WordNetLemmatizer()
 word_list = ["Study","Studying","Studies","Studied"]
 for w in word_list:
     print(lemma.lemmatize(w ,pos="v"))
 from nltk import WordNetLemmatizer
 lemma = WordNetLemmatizer()
 word_list = ["am","is","are","was","were"]
 for w in word_list:
     print(lemma.lemmatize(w ,pos="v"))
 from nltk.stem import PorterStemmer
 stemmer = PorterStemmer()
 print(stemmer.stem('studies'))
 from nltk.stem import WordNetLemmatizer
 lemmatizer = WordNetLemmatizer()
 print(lemmatizer.lemmatize('studies'))
 from nltk.stem import WordNetLemmatizer
 lemmatizer = WordNetLemmatizer()
 print(lemmatizer.lemmatize('studying', pos="v"))
 print(lemmatizer.lemmatize('studying', pos="n"))
 print(lemmatizer.lemmatize('studying', pos="a"))
 print(lemmatizer.lemmatize('studying', pos="r"))
 from nltk import WordNetLemmatizer
 lemma = WordNetLemmatizer()
 word_list = ["studies","leaves","decreases","plays"]
 for w in word_list:
     print(lemma.lemmatize(w))
 tag = nltk.pos_tag(["Studying","Study"])
 print (tag)
 sentence = "A very beautiful young lady is walking on the beach"
 tokenized_words = word_tokenize(sentence)
 for words in tokenized_words:
     tagged_words = nltk.pos_tag(tokenized_words)
 tagged_words
 grammar = "NP : {<DT>?<JJ>*<NN>} "
 import matplotlib.pyplot as plt
 parser = nltk.RegexpParser(grammar)
 output = parser.parse(tagged_words)
 print (output)
 grammar = r""" NP: {<.*>+} 
                }<JJ>+{"""
 parser = nltk.RegexpParser(grammar)
 output = parser.parse(tagged_words)
 print(output)
 sentence = "Mr. Smith made a deal on a beach of Switzerland near WHO."
 tokenized_words = word_tokenize(sentence)
 for w in tokenized_words:
     tagged_words = nltk.pos_tag(tokenized_words)
 N_E_R = nltk.ne_chunk(tagged_words,binary=False)
 print(N_E_R)
 sentence = "Mr. Smith made a deal on a beach of Switzerland near WHO."
 tokenized_words = word_tokenize(sentence)
 for w in tokenized_words:
     tagged_words = nltk.pos_tag(tokenized_words)
 N_E_R = nltk.ne_chunk(tagged_words,binary=True)
 print(N_E_R)
 from nltk.corpus import wordnet
 for words in wordnet.synsets("Fun"): 
     print(words)      
 for words in wordnet.synsets("Fun"): 
     print(words.name())
     print(words.definition())
     print(words.examples())
     for lemma in words.lemmas(): 
         print(lemma)
     print("\n")
 for words in wordnet.synsets("Fun"): 
     for lemma in words.lemmas(): 
         print(lemma)
     print("\n")
 word = wordnet.synsets("Play")[0]
 print(word.name())
 print(word.definition())
 print(word.examples())
 word = wordnet.synsets("Play")[0]
 print(word.hypernyms())
 word = wordnet.synsets("Play")[0]
 word.hyponyms()
 word = wordnet.synsets("Play")[0]
 print(word.lemmas()[0].name())
 synonyms = []
 for words in wordnet.synsets('Fun'):
     for lemma in words.lemmas():
         synonyms.append(lemma.name())
 print(synonyms)
 antonyms = []
 for words in wordnet.synsets('Natural'):
     for lemma in words.lemmas():
         if lemma.antonyms():
             antonyms.append(lemma.antonyms()[0].name())
 print(antonyms)
 synonyms = []
 antonyms = []
 for words in wordnet.synsets('New'):
     for lemma in words.lemmas():
         synonyms.append(lemma.name())
         if lemma.antonyms():
             antonyms.append(lemma.antonyms()[0].name())
 print(synonyms)
 print("\n")
 print(antonyms)
 word1 = wordnet.synsets("ship","n")[0]
 word2 = wordnet.synsets("boat","n")[0] 
 print(word1.wup_similarity(word2)) 
 word1 = wordnet.synsets("ship","n")[0]
 word2 = wordnet.synsets("bike","n")[0] 
 print(word1.wup_similarity(word2)) 
 from sklearn.feature_extraction.text import CountVectorizer
 sentences = ["Jim and Pam travelled by the bus:",
              "The train was late",
              "The flight was full.Travelling by flight is expensive"]
 cv = CountVectorizer()
 B_O_W = cv.fit_transform(sentences).toarray()
 print(cv.vocabulary_)
 print("\n")
 print(cv.get_feature_names())
 print("\n")
 print(B_O_W)
 from sklearn.feature_extraction.text import TfidfVectorizer
 sentences = ['This is the first document','This document is the second document']
 vectorizer = TfidfVectorizer(norm = None)
 X = vectorizer.fit_transform(sentences).toarray()
 print(vectorizer.vocabulary_)
 print("\n")
 print(vectorizer.get_feature_names())
 print("\n")
 print(X)
