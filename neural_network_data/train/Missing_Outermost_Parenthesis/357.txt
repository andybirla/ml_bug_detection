os.chdir'\\'.join(__file__.split('/')[:-1])
def get_page:
	url = input'Enter url of a medium article: '
	if not re.matchr'https?://medium.com/',url:
		print'Please enter a valid website, or make sure it is a medium article'
		sys.exit1
	res = requests.geturl
	res.raise_for_status
	soup = BeautifulSoupres.text, 'html.parser'
def purifytext:
    rep = dict(re.escape(k), v) for k, v in rep.items() 
    pattern = re.compile"|".join(rep.keys())
    text = pattern.sublambda m: rep[re.escape(m.group(0))], text
    text = re.sub'\<(.*?)\>', '', text
def collect_textsoup:
	main = soup.head.title.text.split('|')
	title = main[0].strip
	fin += f'Title: {title.upper}\n{main[1].strip()}'
	header = soup.find_all'h1'
		for elem in listheader[j].previous_siblings[::-1]:
			fin += f'\n{purifystr(elem)}'
	fin += f'\n\n{header[j].text.upper}'
			fin += f'\n\n{header[j].text.upper}'
		fin += f'\n{purifystr(elem)}'
def save_filefin:
	if not os.path.exists'./scraped_articles':
		os.mkdir'./scraped_articles'
	fname = './scraped_articles/' + '_'.jointitle.split() + '.txt'
	with openfname, 'w', encoding='utf8' as outfile:
		outfile.writefin
	printf'File saved in directory {fname}'
	fin = collect_textget_page()
	save_filefin
